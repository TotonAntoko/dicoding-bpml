# -*- coding: utf-8 -*-
"""Submission 1 - Klasifikasi Berita.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qX9Nv00moN_9LCQ8f1yvofxUsN8Kkj0V

# **Konfigurasi Colab dan download dataset**
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/drive/My Drive/Dataset/Kaggle"

# Commented out IPython magic to ensure Python compatibility.
#changing the working directory
# %cd /content/drive/My Drive/Dataset/Kaggle

!kaggle datasets download -d uciml/news-aggregator-dataset

!unzip news-aggregator-dataset.zip -d '/content/drive/My Drive/Dicoding/Submission_1/dataset'

dataset = '/content/drive/My Drive/Dicoding/Submission_1/dataset/uci-news-aggregator.csv'

"""# **Import Libary**"""

import os
import re, string
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from keras.utils import np_utils
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.initializers import Constant
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence

"""# **Define Function**"""

sns.set(style="darkgrid")

# Plot Kategori Distribusi
def plot_distribusi_kategori(title, df):
  fig, ax = plt.subplots(figsize=(10, 8))
  ax.set_title(title)
  ax.set_xlabel('Total')
  sns.countplot(ax=ax, y="CATEGORY", 
                data=df, color='green', 
                order = df['CATEGORY'].value_counts().index, 
                saturation=0.5)

# Plot Distribusi Kolom
def plot_distribusi_kolom(title, df, kolom):
  category = df["CATEGORY"].unique()
  average = []

  for i in category:
    average.append(np.mean(df[df["CATEGORY"] == i][kolom].apply(lambda x : len(x.split()))))

  df_average = pd.DataFrame({
      "CATEGORY" : category,
      "average": average
      })

  sns.set(style="darkgrid")
  fig, ax = plt.subplots(figsize=(10, 8))
  ax.set_title(title)
  ax.set_xlabel('Total')
  sns.barplot(y="CATEGORY", x="average",
              order = df_average.sort_values('average', ascending=False)['CATEGORY'],  
              data=df_average, color='green', saturation=0.5)



"""# **Eksplorasi Data**"""

data = pd.read_csv(dataset)
display(data.head())
print('Jumlah sampel data : {}'.format(data.shape[0]))
print('Jumlah fitur/kolom : {}'.format(data.shape[1]))

print('Jumlah null data :\n', data.isnull().sum())

plot_distribusi_kategori('Kategori Distribusi berdasarkan jumlah berita', data)

"""Ubah nama kategori"""

data['CATEGORY'] = data['CATEGORY'].replace({
    'b' : 'business',
    't' : 'science and technology',
    'e' : 'entertainment',
    'm' : 'health'
})



plot_distribusi_kategori('Kategori Distribusi setelah penggabungan kategori', data)



plot_distribusi_kolom('Average Panjang Kata dalam kolom title', data, 'TITLE')



"""# **Preparing Data**

Memilih jumlah maksimal kategori paling banyak
"""

# Mengambil maksimum data dari setiap kategori
num_of_categories = 35000

# Mengacak datafram
shuffled = data.reindex(np.random.permutation(data.index))
# memecah setiap kategori sebanyak maks 45000
business = shuffled[shuffled['CATEGORY'] == 'business'][:num_of_categories]
science = shuffled[shuffled['CATEGORY'] == 'science and technology'][:num_of_categories]
entertainment = shuffled[shuffled['CATEGORY'] == 'entertainment'][:num_of_categories]
health = shuffled[shuffled['CATEGORY'] == 'health'][:num_of_categories]
# menggabungkan semua kategori
data = pd.concat([business,science,entertainment,health], ignore_index=True)
# mengacak ulang datafram
data = data.reindex(np.random.permutation(data.index))



# Ubah kategori ke bentuk id atau angka
categories = data.groupby('CATEGORY').size().index.tolist()
category_int = {}
int_category = {}
for i, k in enumerate(categories):
    category_int.update({k:i})
    int_category.update({i:k})

data['c2id'] = data['CATEGORY'].apply(lambda x: category_int[x])

# panjang kata
n_most_common_words = 8000

# Tokenizing
tokenizer = Tokenizer(num_words=n_most_common_words, filters=string.punctuation, lower=True)
tokenizer.fit_on_texts(data['TITLE'])
X = tokenizer.texts_to_sequences(data['TITLE'])
data['words'] = X

data['word_length'] = data.words.apply(lambda i: len(i))

data.head()

"""## **Split Data**"""

# maks panjang padding
max_len = 150
features = pad_sequences(data.words, maxlen=max_len)
labels = np_utils.to_categorical(list(data.c2id))
data_train, data_test, label_train, label_test = train_test_split(features, labels, test_size=0.2, random_state=42)
data_train_1, data_test_1, label_train_1, label_test_1 = train_test_split(data, labels, test_size=0.2, random_state=42)



"""# **Plot Distribusi Data Training dan Testing**"""

fig, ax = plt.subplots(figsize=(10, 8))
ax.set_title('title')
ax.set_xlabel('Total')
sns.countplot(ax=ax, y="CATEGORY", 
              data=data_train_1, color='green', 
              order = data_train_1['CATEGORY'].value_counts().index, 
              saturation=0.5)
sns.countplot(ax=ax, y="CATEGORY", 
              data=data_test_1, color='red', 
              order = data_test_1['CATEGORY'].value_counts().index, 
              saturation=0.5)



"""# **Build Model**"""

BATCH_SIZE = 256
EPOCHS = 25
emb_dim = 128

model = Sequential()
model.add(Embedding(n_most_common_words, emb_dim, input_length=data_train.shape[1]))
model.add(SpatialDropout1D(0.4, ))
model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.4, return_sequences=True))
model.add(LSTM(64, dropout=0.4, recurrent_dropout=0.4))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

history = model.fit(
  data_train, 
  label_train, 
  batch_size=BATCH_SIZE, 
  epochs=EPOCHS, 
  validation_split=0.2,
  callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)]
)



"""# **Plot Model**"""

#Summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#Summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()



"""# **Evaluasi Model with unseen data**"""

# Evaluating the model against the test data
results = model.evaluate(data_test, label_test, verbose=1);
print('Uji data menggunakan data yang tidak terlihat\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(results[0],results[1]))